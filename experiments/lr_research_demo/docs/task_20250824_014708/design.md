# Software Design Document (design.md)

1. System Architecture

- Architecture: Single-process CLI application (Python 3.11) with a deterministic dataset generator and an experiment runner that trains, evaluates, and compares linear regression runs. It reads/writes only under the project root. No network or GPU dependencies.
- Data flow:
  - scripts/run_lr.py resolves the project root (FR-001), validates/creates data and experiments directories (FR-008, TR-010), generates/loads the dataset (FR-002, FR-003, FR-005, FR-009), trains/evaluates LinearRegression (FR-010—FR-012), writes artifacts (FR-013—FR-018), prints a compact JSON line (FR-019), updates a comparison report (FR-021—FR-024), and ensures reproducibility (FR-025, FR-026).
- Requirements fulfilled:
  - Functional: FR-001 to FR-030
  - Technical: TR-001 to TR-013
  - Non-Functional: NFR-001 to NFR-008


2. Technology & Tooling Stack

- Programming language:
  - Python 3.11 (TR-001)
- Libraries (runtime):
  - numpy 1.26.4 (TR-003)
  - pandas 2.2.2 (TR-005)
  - scikit-learn 1.4.2 (TR-004)
- Build/package:
  - pip 24.2
- Linters/formatters (dev):
  - ruff 0.6.1
  - black 24.8.0
- Test framework and coverage (dev):
  - pytest 8.3.2
  - coverage.py 7.6.1
- Runtime/process:
  - Python CLI (no additional process manager)
  - Logging via Python logging module with UTC formatting (TR-011)
- Data stores and infra:
  - Local filesystem only; no external services (TR-009, TR-010)
- DevOps & CI/CD:
  - None required
- OS assumptions and system packages:
  - macOS 12+ or Linux x86_64 (TR-009)
  - No additional system packages required


3. File Structure

```text
.
├── .gitignore
├── README.md
├── requirements.md
├── requirements.txt
├── design.md
├── runner.py
├── scripts/
│   └── run_lr.py
├── data/                     # created at runtime if missing
│   ├── dataset.csv           # created by generate_dataset()
│   └── dataset_meta.json     # created by generate_dataset()
└── experiments/              # created at runtime if missing
    ├── lr_run_1/             # created by first experiment run
    │   ├── config.json
    │   ├── metrics.json
    │   ├── coefficients.json
    │   └── logs.txt
    ├── lr_run_2/             # created by second experiment run
    │   ├── config.json
    │   ├── metrics.json
    │   ├── coefficients.json
    │   └── logs.txt
    └── lr_runs_comparison.json
```

Notes:
- Only scripts/run_lr.py will be implemented/updated; all other runtime artifacts are generated by the script per FR-006, FR-013, FR-021.
- No new top-level folders are introduced beyond data and experiments (TR-013).


4. Component & API Design

File: scripts/run_lr.py

- Purpose: Implements deterministic dataset generation (FR-002—FR-006, FR-027), experiment run (FR-007—FR-026, FR-028—FR-030), CLI (TR-002), logging (TR-011), JSON I/O (TR-012), and comparison report (FR-021—FR-024).

- Constants:
  - FEATURE_NAMES: list[str] = ["f1", "f2", "f3"] (FR-002, FR-011)
  - TARGET_NAME: str = "y" (FR-002, FR-011)
  - DATASET_FILENAME: str = "dataset.csv" (FR-005)
  - DATASET_META_FILENAME: str = "dataset_meta.json" (FR-006)
  - MODEL_TYPE: str = "LinearRegression" (FR-014, FR-016)
  - CORR_BOUNDS: dict = {"f1_f2": (0.80, 0.95), "f1_f3": (-0.20, 0.20), "f2_f3": (-0.20, 0.20)} (FR-004)
  - CORR_ATOL: float = 1e-12 (NFR-004)

- Exceptions:
  - class ArgError(Exception): pass  # mapped to exit code 2 (FR-020)
  - class DataError(Exception): pass  # mapped to exit code 3 (FR-009, FR-004)
  - class TrainError(Exception): pass  # mapped to exit code 4 (FR-020)
  - class EvalError(Exception): pass  # mapped to exit code 5 (FR-020)
  - class WriteError(Exception): pass  # mapped to exit code 6 (FR-020)

- Functions and signatures:

  - def get_project_root() -> pathlib.Path
    - Description: Locate the absolute project root as the directory containing requirements.md (FR-001). Searches upward from current working directory. If not found, raises DataError; optionally, to maximize compatibility, fallback to current working directory only if requirements.md exists there.
    - Requirements: FR-001, TR-010

  - def resolve_and_validate_subpath(root: pathlib.Path, path_str: str) -> pathlib.Path
    - Description: Resolve path_str to an absolute path, normalize, ensure it is within root using Path.is_relative_to; otherwise raise ArgError (FR-008, TR-010).
    - Returns: resolved absolute Path in root
    - Requirements: FR-008, TR-010

  - def utc_now_iso() -> str
    - Description: Return current UTC ISO 8601 string with microseconds and trailing Z (e.g., 2025-08-24T01:23:45.678901Z) (TR-008). Note: Not used for artifacts to preserve reproducibility (NFR-003, FR-025); used only for logs if needed.

  - def deterministic_timestamp_from_path(path: pathlib.Path) -> str
    - Description: Compute a deterministic ISO 8601 UTC timestamp from file mtime with microseconds and a trailing Z, using st_mtime_ns to maximize stability. This is used for config.json and metrics.json timestamps to ensure byte-identical outputs across re-runs when dataset is unchanged (FR-025, NFR-003, TR-008).
    - Returns: str

  - def write_json_file(path: pathlib.Path, obj: dict) -> None
    - Description: Write JSON with UTF-8 encoding, indent=2, ensure_ascii=False, separators=(", ", ": "), and a trailing newline (TR-012, FR-017). If write fails, raise WriteError (FR-020).
    - Requirements: TR-012, FR-017, FR-020

  - def read_json_file(path: pathlib.Path) -> dict
    - Description: Strict JSON load, UTF-8 encoding. Raises DataError on failure.

  - def generate_dataset(config: dict) -> dict
    - Signature: generate_dataset(config: dict) -> dict
    - Input schema: FR-027
    - Return schema: FR-027
    - Behavior:
      - Resolve project root (FR-001); resolve/validate data_dir (FR-008).
      - If dataset.csv exists and force_regenerate is False: return paths and generated=False (FR-003).
      - Else, generate with numpy.random.default_rng(seed=42) (TR-007) exactly as FR-005, compute Pearson correlations for full dataset, validate bounds with atol=1e-12 (FR-004, NFR-004).
      - If validation fails: raise DataError (exit code 3) (FR-020).
      - Save dataset to data/dataset.csv with header order f1,f2,f3,y, comma separators, LF line endings, no index (FR-005, TR-006).
      - Write data/dataset_meta.json according to FR-006; overwrite whenever (re)generated (FR-006).
      - Return {"dataset_path": str, "meta_path": str, "generated": True}.
    - Requirements: FR-002—FR-006, FR-027, TR-003, TR-005, TR-006, TR-007

    Pseudocode:
    ```
    def generate_dataset(config: dict) -> dict:
        root = get_project_root()
        data_dir = resolve_and_validate_subpath(root, config["data_dir"])
        dataset_path = data_dir / DATASET_FILENAME
        meta_path = data_dir / DATASET_META_FILENAME

        if dataset_path.exists() and not config.get("force_regenerate", False):
            return {"dataset_path": str(dataset_path), "meta_path": str(meta_path), "generated": False}

        try:
            data_dir.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            raise DataError(f"Cannot create data dir: {e}") from e

        # Deterministic RNG
        rng = np.random.default_rng(seed=42)  # TR-007
        n = 1000
        z = rng.normal(0.0, 1.0, size=n)
        e1 = rng.normal(0.0, 0.2, size=n)
        e2 = rng.normal(0.0, 0.2, size=n)
        e3 = rng.normal(0.0, 1.0, size=n)
        f1 = z + e1
        f2 = z + e2
        f3 = e3
        noise = rng.normal(0.0, 1.0, size=n)
        y = 5.0 + 3.0 * f1 - 2.0 * f2 + 0.5 * f3 + noise

        # Float64 enforcement (TR-006)
        f1 = f1.astype(np.float64)
        f2 = f2.astype(np.float64)
        f3 = f3.astype(np.float64)
        y = y.astype(np.float64)

        # Correlations (FR-004)
        def pearson(a, b):
            a = a.astype(np.float64)
            b = b.astype(np.float64)
            c = np.corrcoef(a, b)[0, 1]
            return float(c)

        r12 = pearson(f1, f2)
        r13 = pearson(f1, f3)
        r23 = pearson(f2, f3)

        def in_range(val, lo, hi):
            return not (val + CORR_ATOL < lo or val - CORR_ATOL > hi)

        if not in_range(r12, 0.80, 0.95) or not in_range(r13, -0.20, 0.20) or not in_range(r23, -0.20, 0.20):
            raise DataError(f"Correlation out of bounds: f1_f2={r12}, f1_f3={r13}, f2_f3={r23}")

        # Save CSV (FR-005)
        df = pd.DataFrame({"f1": f1, "f2": f2, "f3": f3, "y": y}, dtype=np.float64)
        try:
            df.to_csv(dataset_path, index=False, lineterminator="\n", encoding="utf-8")
        except Exception as e:
            raise WriteError(f"Cannot write dataset: {e}") from e

        # Save metadata (FR-006)
        meta = {
            "n_rows": 1000,
            "n_features": 3,
            "feature_names": ["f1", "f2", "f3"],
            "target_name": "y",
            "generation_seed": 42,
            "correlations": {
                "pearson_f1_f2": r12,
                "pearson_f1_f3": r13,
                "pearson_f2_f3": r23,
            },
            "coefficients": {"f1": 3.0, "f2": -2.0, "f3": 0.5},
            "intercept": 5.0,
            "noise_std": 1.0,
        }
        try:
            write_json_file(meta_path, meta)
        except WriteError:
            # If meta write fails, propagate as exit 6 per FR-020
            raise

        return {"dataset_path": str(dataset_path), "meta_path": str(meta_path), "generated": True}
    ```

  - def run_experiment(config: dict) -> dict
    - Signature: run_experiment(config: dict) -> dict
    - Input schema: FR-028
    - Return schema: FR-028
    - Behavior:
      - Validate arguments: seed int; test_size in (0,1); run_id matches ^[A-Za-z0-9_\-]+$; data_dir and out_dir strings (FR-007, FR-028). On failure raise ArgError (exit code 2).
      - Resolve project root; resolve/validate data_dir and out_dir to be within root; create dirs as needed (FR-008, TR-010).
      - Ensure dataset exists by calling generate_dataset({"data_dir": data_dir, "force_regenerate": False}); on DataError raise and map to exit code 3 (FR-009).
      - Load dataset from dataset.csv with pandas.read_csv, dtype float64; error -> DataError (FR-009, TR-006).
      - Split train/test via sklearn.model_selection.train_test_split with test_size=test_size, shuffle=True, random_state=seed (FR-010, TR-007).
      - Train sklearn.linear_model.LinearRegression() with default params on float64 (FR-011, TR-006).
      - Evaluate MSE and R2 on test set (FR-012).
      - Prepare artifacts directory {out_dir}/{run_id}; create if missing (FR-013).
      - Compute deterministic timestamp from dataset file mtime (deterministic_timestamp_from_path) to ensure byte-identical artifacts on re-run with same inputs (FR-025, NFR-003, TR-008).
      - Write config.json, metrics.json, coefficients.json with required schemas and ordered keys (FR-014—FR-017, TR-012). Append logs.txt line via logging FileHandler (FR-018, TR-011).
      - Print a single-line compact JSON to stdout exactly matching metrics.json content (FR-019).
      - Update {out_dir}/lr_runs_comparison.json (FR-021—FR-024).
      - Return result dict with artifact paths and parsed stdout JSON (FR-028).
    - Requirements: FR-007—FR-026, FR-028—FR-030, TR-002, TR-006—TR-013, NFR-001—NFR-008

    Pseudocode:
    ```
    def run_experiment(config: dict) -> dict:
        # Validate inputs (FR-007, FR-028)
        seed = int(config["seed"])
        test_size = float(config.get("test_size", 0.2))
        run_id = config.get("run_id") or f"lr_run_{seed}"
        data_dir_str = config.get("data_dir", "./data")
        out_dir_str = config.get("out_dir", "./experiments")

        if not (0.0 < test_size < 1.0):
            raise ArgError("test_size must be in (0,1)")
        if not re.match(r"^[A-Za-z0-9_\-]+$", run_id):
            raise ArgError("run_id has invalid characters")

        root = get_project_root()
        data_dir = resolve_and_validate_subpath(root, data_dir_str)
        out_dir = resolve_and_validate_subpath(root, out_dir_str)

        try:
            data_dir.mkdir(parents=True, exist_ok=True)
            out_dir.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            raise DataError(f"Cannot create directories: {e}")

        # Dataset ensure/generate (FR-009)
        ds_result = generate_dataset({"data_dir": str(data_dir), "force_regenerate": False})
        dataset_path = pathlib.Path(ds_result["dataset_path"])

        # Load dataset (TR-006)
        try:
            df = pd.read_csv(dataset_path, dtype=np.float64)
        except Exception as e:
            raise DataError(f"Failed to read dataset: {e}") from e

        # Prepare features/target
        X = df[FEATURE_NAMES].to_numpy(dtype=np.float64)
        y = df[TARGET_NAME].to_numpy(dtype=np.float64)

        # Split (FR-010)
        try:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, shuffle=True, random_state=seed
            )
        except Exception as e:
            raise ArgError(f"Split failed: {e}") from e

        # Train (FR-011)
        try:
            model = LinearRegression()
            model.fit(X_train, y_train)
        except Exception as e:
            raise TrainError(f"Training failed: {e}") from e

        # Evaluate (FR-012)
        try:
            y_pred = model.predict(X_test)
            mse = mean_squared_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
        except Exception as e:
            raise EvalError(f"Evaluation failed: {e}") from e

        # Artifacts (FR-013—FR-017)
        artifacts_dir = out_dir / run_id
        try:
            artifacts_dir.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            raise WriteError(f"Cannot create artifacts dir: {e}")

        # Deterministic timestamp (FR-025, NFR-003, TR-008)
        ts = deterministic_timestamp_from_path(dataset_path)

        # Paths
        config_path = artifacts_dir / "config.json"
        metrics_path = artifacts_dir / "metrics.json"
        coeffs_path = artifacts_dir / "coefficients.json"
        logs_path = artifacts_dir / "logs.txt"

        # Logging to file only (TR-011, FR-018)
        logger = logging.getLogger("lr")
        logger.handlers.clear()
        logger.setLevel(logging.INFO)
        class UtcFormatter(logging.Formatter):
            converter = time.gmtime
        fh = logging.FileHandler(str(logs_path), encoding="utf-8")
        fh.setLevel(logging.INFO)
        fh.setFormatter(UtcFormatter("%(asctime)s %(levelname)s %(message)s"))
        logger.addHandler(fh)
        logger.propagate = False

        # config.json (FR-014)
        cfg = {
            "run_id": run_id,
            "seed": seed,
            "data_dir": str(data_dir),
            "out_dir": str(out_dir),
            "test_size": test_size,
            "model_type": MODEL_TYPE,
            "dataset_path": str(dataset_path),
            "timestamp_utc": ts,
        }
        write_json_file(config_path, cfg)

        # coefficients.json (FR-015)
        coef_map = {name: float(model.coef_[i]) for i, name in enumerate(FEATURE_NAMES)}
        coeffs = {
            "intercept": float(model.intercept_),
            "coef": coef_map,
        }
        write_json_file(coeffs_path, coeffs)

        # metrics.json (FR-016)
        metrics_obj = {
            "run_id": run_id,
            "seed": seed,
            "model_type": MODEL_TYPE,
            "metrics": {"mse": float(mse), "r2": float(r2)},
            "counts": {"train_rows": int(y_train.shape[0]), "test_rows": int(y_test.shape[0]), "n_features": 3},
            "timestamp_utc": ts,
        }
        write_json_file(metrics_path, metrics_obj)

        # Plain-text log line (FR-018)
        logger.info(
            "dataset=%s run_id=%s seed=%d test_size=%.6f train_rows=%d test_rows=%d mse=%.*g r2=%.*g",
            str(dataset_path), run_id, seed, test_size, y_train.shape[0], y_test.shape[0], 17, mse, 17, r2
        )

        # Stdout single-line JSON (FR-019)
        stdout_line = json.dumps(metrics_obj, ensure_ascii=False, separators=(",", ":"))
        sys.stdout.write(stdout_line)
        sys.stdout.flush()

        # Update comparison report (FR-021—FR-024)
        update_comparison_report(out_dir)

        return {
            "artifacts_dir": str(artifacts_dir),
            "config_path": str(config_path),
            "metrics_path": str(metrics_path),
            "coefficients_path": str(coeffs_path),
            "stdout_json": metrics_obj,
        }
    ```

  - def update_comparison_report(out_dir: pathlib.Path) -> pathlib.Path
    - Description: Scan subdirectories of out_dir starting with "lr_run_" containing metrics.json, aggregate, compute rankings, best_by, and pairwise deltas per FR-021—FR-024, write lr_runs_comparison.json (TR-012).
    - Returns: Path to comparison JSON
    - Requirements: FR-021—FR-024, NFR-006

    Pseudocode:
    ```
    def update_comparison_report(out_dir: pathlib.Path) -> pathlib.Path:
        runs = []
        for child in out_dir.iterdir():
            if child.is_dir() and child.name.startswith("lr_run_"):
                mp = child / "metrics.json"
                if mp.exists():
                    try:
                        m = read_json_file(mp)
                    except Exception:
                        continue
                    runs.append({
                        "run_id": str(m.get("run_id")),
                        "seed": int(m.get("seed")),
                        "mse": float(m["metrics"]["mse"]),
                        "r2": float(m["metrics"]["r2"]),
                    })

        # Rankings (FR-023)
        ranking_by_mse = [r["run_id"] for r in sorted(runs, key=lambda x: x["mse"])]
        ranking_by_r2 = [r["run_id"] for r in sorted(runs, key=lambda x: x["r2"], reverse=True)]
        best_by_mse = ranking_by_mse[0] if ranking_by_mse else ""
        best_by_r2 = ranking_by_r2[0] if ranking_by_r2 else ""

        # Pairwise (FR-024)
        pairwise = {}
        for a in runs:
            for b in runs:
                if a["run_id"] == b["run_id"]:
                    continue
                key = f"{a['run_id']}_vs_{b['run_id']}"
                pairwise[key] = {
                    "delta_mse": float(a["mse"] - b["mse"]),
                    "delta_r2": float(a["r2"] - b["r2"]),
                }

        report = {
            "runs": runs,
            "ranking_by_mse": ranking_by_mse,
            "ranking_by_r2": ranking_by_r2,
            "best_by_mse": best_by_mse,
            "best_by_r2": best_by_r2,
            "pairwise": pairwise,
        }
        path = out_dir / "lr_runs_comparison.json"
        write_json_file(path, report)
        return path
    ```

  - def parse_args(argv: list[str] | None = None) -> argparse.Namespace
    - Description: argparse CLI per FR-007 with options:
      - --seed (required, int)
      - --data-dir (optional, default "./data")
      - --out-dir (optional, default "./experiments")
      - --test-size (optional, float in (0,1), default 0.2)
      - --run-id (optional, default "lr_run_{seed}")
    - On validation failures, main maps to exit code 2 (FR-020).

  - def main(argv: list[str] | None = None) -> int
    - Description: Entrypoint to parse args, invoke run_experiment(), and map exceptions to exit codes (FR-020, FR-029). Returns process exit code.
    - Behavior:
      - Try run_experiment; on success return 0.
      - Except ArgError -> return 2; DataError -> 3; TrainError -> 4; EvalError -> 5; WriteError -> 6.
      - Ensure no stdout pollution other than the single-line JSON (FR-019). Do not attach root logger handlers that output to stdout.


5. Dependencies

Runtime dependencies (to be pinned in requirements.txt):
- numpy==1.26.4
- pandas==2.2.2
- scikit-learn==1.4.2

Dev dependencies (optional; installed separately or in a dev requirements file):
- pytest==8.3.2
- coverage==7.6.1
- ruff==0.6.1
- black==24.8.0

System/Tooling:
- Python==3.11.x
- pip==24.2

Note: No GPU, no additional OS packages required. All numeric operations use float64 (TR-006).


6. Implementation Plan

1) Setup Python environment (TR-001)
- Commands:
  - python3.11 -m venv .venv
  - source .venv/bin/activate  # Windows: .venv\Scripts\activate
  - python -m pip install --upgrade pip==24.2
  - pip install -r requirements.txt

2) Implement scripts/run_lr.py (FR-007, FR-027—FR-029)
- Create or replace scripts/run_lr.py with:
  - Imports: argparse, json, logging, os, pathlib, re, sys, time, datetime; numpy as np; pandas as pd; sklearn.model_selection.train_test_split; sklearn.linear_model.LinearRegression; sklearn.metrics.mean_squared_error, r2_score.
  - Constants, exceptions, helper functions per Component & API Design.
  - Implement generate_dataset(config) exactly as pseudocode (FR-005, FR-006).
  - Implement run_experiment(config) exactly as pseudocode; enforce float64; write artifacts; print stdout compact JSON; update comparison.
  - Implement update_comparison_report(out_dir).
  - Implement parse_args and main; call sys.exit(main()) under if __name__ == "__main__": guard.

3) Validate CLI parsing and argument checks (FR-007, FR-020)
- Run:
  - python scripts/run_lr.py --help
  - python scripts/run_lr.py --seed 1 --test-size 0.2  # expect success

4) First experiment run (FR-002—FR-006, FR-009—FR-019, FR-021)
- Run:
  - python scripts/run_lr.py --seed 1
- Expected:
  - data/dataset.csv and data/dataset_meta.json created
  - experiments/lr_run_1/{config.json, metrics.json, coefficients.json, logs.txt} created
  - experiments/lr_runs_comparison.json created

5) Second experiment run (FR-026, FR-030, FR-021—FR-024)
- Run:
  - python scripts/run_lr.py --seed 2
- Expected:
  - experiments/lr_run_2/... created
  - experiments/lr_runs_comparison.json updated with both runs; rankings computed.

6) Reproducibility check (FR-025, NFR-003)
- Run twice:
  - python scripts/run_lr.py --seed 1
  - python scripts/run_lr.py --seed 1
- Compare bytes:
  - diff experiments/lr_run_1/metrics.json experiments/lr_run_1/metrics.json  # should be identical across runs
  - Similarly for config.json, coefficients.json.

7) Lint/format (optional dev)
- Commands:
  - ruff scripts/run_lr.py
  - black scripts/run_lr.py

8) Performance check (NFR-001, NFR-002, NFR-006, NFR-007)
- Confirm runtime < 5s per run and memory < 256 MB.
- Confirm lr_runs_comparison.json update is fast (<200ms for up to 50 runs).
- Confirm stdout JSON line size < 2 KB.

9) Error handling verification (FR-020)
- Try invalid args:
  - python scripts/run_lr.py --seed 1 --test-size 0    # exit 2
- Temporarily restrict permissions on data to test exit 3, 6 (optional).


7. Testing Strategy

- Frameworks:
  - pytest 8.3.2
  - coverage.py 7.6.1
- Scope:
  - Unit tests (if authored locally without creating a new top-level tests/ dir, execute ad-hoc or within scripts/ prefixed with test_*.py then deleted; note TR-013 discourages new top-level folders).
  - Manual verification via commands below.

- Key test cases:
  - Dataset generation deterministic: After first run, data/dataset.csv exists; running again does not regenerate (generated=False) (FR-003).
  - Correlation bounds: Verify metadata correlations within specified intervals (FR-004).
  - CLI and artifacts: For seeds 1 and 2, metrics.json matches stdout JSON exactly; artifacts are present (FR-013—FR-019).
  - Reproducibility: Re-run same command; verify byte-identical JSON artifacts (FR-025, NFR-003).
  - Comparison report correctness: rankings by MSE ascending, by R2 descending; pairwise deltas exist for all ordered pairs (FR-023, FR-024).

- Commands:
  - Run experiments:
    - python scripts/run_lr.py --seed 1
    - python scripts/run_lr.py --seed 2
  - Optional coverage (if tests present):
    - coverage run -m pytest -q
    - coverage report --fail-under=85

- Coverage thresholds (if tests present):
  - Statement coverage >= 85% for scripts/run_lr.py


8. Local Development & Environment

- Environment setup:
  - Python 3.11 required
  - Virtual environment:
    - python3.11 -m venv .venv
    - source .venv/bin/activate  # Windows: .venv\Scripts\activate
    - python -m pip install --upgrade pip==24.2
    - pip install -r requirements.txt

- requirements.txt (must contain):
  - numpy==1.26.4
  - pandas==2.2.2
  - scikit-learn==1.4.2

- Dev tools (optional):
  - pip install pytest==8.3.2 coverage==7.6.1 ruff==0.6.1 black==24.8.0

- Running the project:
  - Run lr_run_1: python scripts/run_lr.py --seed 1
  - Run lr_run_2: python scripts/run_lr.py --seed 2
  - Optional custom dirs: python scripts/run_lr.py --seed 3 --data-dir ./data --out-dir ./experiments --test-size 0.2 --run-id lr_run_3

- Environment variables:
  - None required; no .env usage.
  - All paths are validated to be subpaths of the project root (TR-010).

- Notes:
  - The project root is auto-detected as the absolute directory containing requirements.md (FR-001). Run commands from the project root to avoid confusion.
  - Logging uses UTC timestamps with format "%(asctime)s %(levelname)s %(message)s" and writes only to logs.txt in the artifact directory (TR-011).
  - JSON files are written with indent=2, ensure_ascii=False, separators=(", ", ": "), and a trailing newline; stdout prints a compact single-line JSON (TR-012, FR-017, FR-019).